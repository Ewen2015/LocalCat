{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "426bb138-beb2-4a1d-b1a1-642c749ca2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install langchain -U -i https://pypi.douban.com/simple\n",
    "# pip install sentence-transformers -i https://pypi.douban.com/simple\n",
    "# pip install pydantic==1.10.8 -i https://pypi.douban.com/simple\n",
    "# pip install chromadb #-i https://pypi.douban.com/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f738961-6a22-4966-b9fa-7f202df05a35",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "## Internet\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/643233392\n",
    "\n",
    "### 1. Get the Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26066e1f-d745-4d00-9bb5-0dcca9584f67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks. For example, it passes a simulated bar exam with a score around the top 10% of test takers; in contrast, GPT-3.5’s score was around the bottom 10%. We’ve spent 6 months iteratively aligning GPT-4 using lessons from our adversarial testing program as well as ChatGPT, resulting in our best-ever results (though far from perfect) on factuality, steerability, and refusing to go outside of guardrails.Over the past two years, we rebuilt our entire deep learning stack and, together with Azure, co-designed a supercomputer from the ground up for our workload. A year ago, we trained GPT-3.5 as a first “test run” of the system. We found and fixed some bugs and improved our theoretical foundations. As a result, our GPT-4 training run was (for us at least!) unprecedentedly stable, becoming our first large model whose training performance we were able to accurately predict ahead of time. As we continue to focus on reliable scaling, we aim to hone our methodology to help us predict and prepare for future capabilities increasingly far in advance—something we view as critical for safety.We are releasing GPT-4’s text input capability via ChatGPT and the API (with a waitlist). To prepare the image input capability for wider availability, we’re collaborating closely with a single partner to start. We’re also open-sourcing OpenAI Evals, our framework for automated evaluation of AI model performance, to allow anyone to report shortcomings in our models to help guide further improvements.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = \"https://en.wikipedia.org/wiki/GPT-4\"\n",
    "url = \"https://openai.com/research/gpt-4\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# find the content div\n",
    "content_div = soup.find('div', {'class': 'ui-block--text'})\n",
    "\n",
    "# remove unwanted elements from div\n",
    "unwanted_tags = ['sup', 'span', 'table', 'ul', 'ol']\n",
    "for tag in unwanted_tags:\n",
    "    for match in content_div.findAll(tag):\n",
    "        match.extract()\n",
    "\n",
    "article_text = content_div.get_text()\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3afc268f-3c08-45f0-9483-8ca39d7edd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\"\"\n",
    "We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks. For example, it passes a simulated bar exam with a score around the top 10% of test takers; in contrast, GPT-3.5’s score was around the bottom 10%. We’ve spent 6 months iteratively aligning GPT-4 using lessons from our adversarial testing program as well as ChatGPT, resulting in our best-ever results (though far from perfect) on factuality, steerability, and refusing to go outside of guardrails.Over the past two years, we rebuilt our entire deep learning stack and, together with Azure, co-designed a supercomputer from the ground up for our workload. A year ago, we trained GPT-3.5 as a first “test run” of the system. We found and fixed some bugs and improved our theoretical foundations. As a result, our GPT-4 training run was (for us at least!) unprecedentedly stable, becoming our first large model whose training performance we were able to accurately predict ahead of time. As we continue to focus on reliable scaling, we aim to hone our methodology to help us predict and prepare for future capabilities increasingly far in advance—something we view as critical for safety.We are releasing GPT-4’s text input capability via ChatGPT and the API (with a waitlist). To prepare the image input capability for wider availability, we’re collaborating closely with a single partner to start. We’re also open-sourcing OpenAI Evals, our framework for automated evaluation of AI model performance, to allow anyone to report shortcomings in our models to help guide further improvements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77af6f2-78d2-4ebc-8fcd-74d708ab77ad",
   "metadata": {},
   "source": [
    "### 2. Split the Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a32e2fc9-2305-4591-bb11-512a5102aef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is'\n",
      "page_content='learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([article_text])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293bfa0e-1bc2-41f5-81d4-663fdca29d85",
   "metadata": {},
   "source": [
    "### 3. Text Chunks to Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84b3b0c6-bb1a-4907-b0e5-6c9544472402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name='shibing624/text2vec-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ff758-b7ca-4fa5-b952-c292376de305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store = FAISS.load_local(vs_path, self.embeddings)\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# use the text chunks and the embeddings model to fill our vector store\n",
    "db = Chroma.from_documents(texts, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c35eb5-4acd-40c3-8a37-e320f1cb0209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "user_question = \"ChatGPT and GPT4, which on is more stable?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b6c0d-3e84-4253-82f9-b4e94c49f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our vector store to find similar text chunks\n",
    "results = db.similarity_search(\n",
    "    query=user_question,\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06acd264-44ac-4313-9f96-8409b1720705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = \"\"\"\n",
    "[Document(page_content='As a result, our GPT-4 training run was (for us at least!) unprecedentedly stable, becoming our'),\n",
    " Document(page_content='GPT-4’s text input capability via ChatGPT and the API (with a waitlist). To prepare the image input'),\n",
    " Document(page_content='We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a'),\n",
    " Document(page_content='GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that,')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb2ecdc-b529-4ca4-a9be-c8e49fbb688d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Document(page_content='As a result, our GPT-4 training run was (for us at least!) unprecedentedly stable, becoming our'),\\n Document(page_content='GPT-4’s text input capability via ChatGPT and the API (with a waitlist). To prepare the image input'),\\n Document(page_content='We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a'),\\n Document(page_content='GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that,')]\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd68dc4c-c56a-479e-a07e-24005ba04e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the prompt template\n",
    "template = \"\"\"\n",
    "You are a chat bot who loves to help people! Given the following context sections, answer the\n",
    "question using only the given context. If you are unsure and the answer is not\n",
    "explicitly writting in the documentation, say \"Sorry, I don't know how to help with that.\"\n",
    "\n",
    "Context sections:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{users_question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"users_question\"])\n",
    "\n",
    "# fill the prompt template\n",
    "prompt_text = prompt.format(context=results, users_question=user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2b25af-dabd-4b70-8b7b-c5247ca634c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='As a result, our GPT-4 training run was (for us at least!) unprecedentedly stable, becoming our'),\n",
       " Document(page_content='GPT-4’s text input capability via ChatGPT and the API (with a waitlist). To prepare the image input'),\n",
       " Document(page_content='We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a'),\n",
       " Document(page_content='GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that,')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e81474-dac0-4cdc-abbd-8733b8405afd",
   "metadata": {},
   "source": [
    "### 4. Go to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c5981c-2070-40bc-b4e6-7457963b7f52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from load_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fa946c6-0748-41b8-8756-5e2f212532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPT-4 is more stable as per the given context.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ask the defined LLM\n",
    "llm(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10740bc4-1a9d-4f6e-8e8c-650efd6c7c83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is difficult to determine which version of GPT (GPT3 or GPT4) is more stable as it depends on the specific context. However, GPT4 has been released recently and has some new features and improvements over GPT3. It is also designed to run more efficiently on hardware, which could make it more stable.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21402ac0-7c2f-4a39-8434-e6e7cd7bd0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
